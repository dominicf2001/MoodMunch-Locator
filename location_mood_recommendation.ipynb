{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> Food Recommendation Base On Emotion for College Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for food dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125 entries, 0 to 124\n",
      "Data columns (total 61 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   GPA                           123 non-null    object \n",
      " 1   Gender                        125 non-null    int64  \n",
      " 2   breakfast                     125 non-null    int64  \n",
      " 3   calories_chicken              125 non-null    int64  \n",
      " 4   calories_day                  106 non-null    float64\n",
      " 5   calories_scone                124 non-null    float64\n",
      " 6   coffee                        125 non-null    int64  \n",
      " 7   comfort_food                  124 non-null    object \n",
      " 8   comfort_food_reasons          123 non-null    object \n",
      " 9   comfort_food_reasons_coded    106 non-null    float64\n",
      " 10  cook                          122 non-null    float64\n",
      " 11  comfort_food_reasons_coded.1  125 non-null    int64  \n",
      " 12  cuisine                       108 non-null    float64\n",
      " 13  diet_current                  124 non-null    object \n",
      " 14  diet_current_coded            125 non-null    int64  \n",
      " 15  drink                         123 non-null    float64\n",
      " 16  eating_changes                122 non-null    object \n",
      " 17  eating_changes_coded          125 non-null    int64  \n",
      " 18  eating_changes_coded1         125 non-null    int64  \n",
      " 19  eating_out                    125 non-null    int64  \n",
      " 20  employment                    116 non-null    float64\n",
      " 21  ethnic_food                   125 non-null    int64  \n",
      " 22  exercise                      112 non-null    float64\n",
      " 23  father_education              124 non-null    float64\n",
      " 24  father_profession             122 non-null    object \n",
      " 25  fav_cuisine                   123 non-null    object \n",
      " 26  fav_cuisine_coded             125 non-null    int64  \n",
      " 27  fav_food                      123 non-null    float64\n",
      " 28  food_childhood                124 non-null    object \n",
      " 29  fries                         125 non-null    int64  \n",
      " 30  fruit_day                     125 non-null    int64  \n",
      " 31  grade_level                   125 non-null    int64  \n",
      " 32  greek_food                    125 non-null    int64  \n",
      " 33  healthy_feeling               125 non-null    int64  \n",
      " 34  healthy_meal                  124 non-null    object \n",
      " 35  ideal_diet                    124 non-null    object \n",
      " 36  ideal_diet_coded              125 non-null    int64  \n",
      " 37  income                        124 non-null    float64\n",
      " 38  indian_food                   125 non-null    int64  \n",
      " 39  italian_food                  125 non-null    int64  \n",
      " 40  life_rewarding                124 non-null    float64\n",
      " 41  marital_status                124 non-null    float64\n",
      " 42  meals_dinner_friend           122 non-null    object \n",
      " 43  mother_education              122 non-null    float64\n",
      " 44  mother_profession             123 non-null    object \n",
      " 45  nutritional_check             125 non-null    int64  \n",
      " 46  on_off_campus                 124 non-null    float64\n",
      " 47  parents_cook                  125 non-null    int64  \n",
      " 48  pay_meal_out                  125 non-null    int64  \n",
      " 49  persian_food                  124 non-null    float64\n",
      " 50  self_perception_weight        124 non-null    float64\n",
      " 51  soup                          124 non-null    float64\n",
      " 52  sports                        123 non-null    float64\n",
      " 53  thai_food                     125 non-null    int64  \n",
      " 54  tortilla_calories             124 non-null    float64\n",
      " 55  turkey_calories               125 non-null    int64  \n",
      " 56  type_sports                   99 non-null     object \n",
      " 57  veggies_day                   125 non-null    int64  \n",
      " 58  vitamins                      125 non-null    int64  \n",
      " 59  waffle_calories               125 non-null    int64  \n",
      " 60  weight                        123 non-null    object \n",
      "dtypes: float64(20), int64(27), object(14)\n",
      "memory usage: 59.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPA                  2\n",
       "Gender               0\n",
       "breakfast            0\n",
       "calories_chicken     0\n",
       "calories_day        19\n",
       "                    ..\n",
       "type_sports         26\n",
       "veggies_day          0\n",
       "vitamins             0\n",
       "waffle_calories      0\n",
       "weight               2\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_recommend = pd.read_csv('dataset/food_coded.csv') \n",
    "food_recommend.info()\n",
    "food_recommend.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Food Types</th>\n",
       "      <th>Emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>we dont have comfort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chocolate, chips, ice cream</td>\n",
       "      <td>Stress, bored, anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frozen yogurt, pizza, fast food</td>\n",
       "      <td>stress, sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pizza, Mac and cheese, ice cream</td>\n",
       "      <td>Boredom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ice cream, chocolate, chips</td>\n",
       "      <td>Stress, boredom, cravings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>wine. mac and cheese, pizza, ice cream</td>\n",
       "      <td>boredom and sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Pizza / Wings / Cheesecake</td>\n",
       "      <td>Loneliness / Homesick / Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>rice, potato, seaweed soup</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Mac n Cheese, Lasagna, Pizza</td>\n",
       "      <td>happiness, they are some of my favorite foods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Chocolates, pizza, and Ritz.</td>\n",
       "      <td>hormones, Premenstrual syndrome.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Food Types  \\\n",
       "0                                       none   \n",
       "1                chocolate, chips, ice cream   \n",
       "2            frozen yogurt, pizza, fast food   \n",
       "3           Pizza, Mac and cheese, ice cream   \n",
       "4               Ice cream, chocolate, chips    \n",
       "..                                       ...   \n",
       "120  wine. mac and cheese, pizza, ice cream    \n",
       "121               Pizza / Wings / Cheesecake   \n",
       "122               rice, potato, seaweed soup   \n",
       "123             Mac n Cheese, Lasagna, Pizza   \n",
       "124             Chocolates, pizza, and Ritz.   \n",
       "\n",
       "                                          Emotions  \n",
       "0                            we dont have comfort   \n",
       "1                             Stress, bored, anger  \n",
       "2                                  stress, sadness  \n",
       "3                                          Boredom  \n",
       "4                       Stress, boredom, cravings   \n",
       "..                                             ...  \n",
       "120                           boredom and sadness   \n",
       "121                Loneliness / Homesick / Sadness  \n",
       "122                                        sadness  \n",
       "123  happiness, they are some of my favorite foods  \n",
       "124               hormones, Premenstrual syndrome.  \n",
       "\n",
       "[125 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_recommend = pd.read_csv('dataset/food_coded.csv', sep=',', usecols=['comfort_food', 'comfort_food_reasons'])\n",
    "\n",
    "# only take two attributes comfort_food and comfort_food_reason and rename\n",
    "food_recommend.rename(columns={'comfort_food': 'Food Types', 'comfort_food_reasons': 'Emotions'}, inplace=True)\n",
    "food_recommend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data clean: clean or fill NaN values in 'comfort_food_reasons' and 'comfort_food' columns\n",
    "\n",
    "#### There is some NaN in the dataset, it could being treat as int. So we need to clean that or fill with empty str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_recommend[\"Emotions\"] = food_recommend[\"Emotions\"].fillna(\"\")\n",
    "food_recommend[\"Food Types\"] = food_recommend[\"Food Types\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## Data Processing with NLTK\n",
    "- Apply NLTK Stopwords to filter out all common words. These are common words used in any language (such as \"the\", \"is\", \"in\", \"and\") that are often filtered out before processing text because they don't have important meaning and are very frequent. \n",
    "    - In this project, we are not only stop common word, but we also extended with some punctuation marks with \"stop.update()\n",
    "    - When processing each emotions_item, we split text into individual words and removes these stopwords. This helps in focusing on words that are more likely to hold specific meaning related to mood\n",
    "- Apply NLTK Lemmatizer to reduce or filter the part in words base on it root. It's the process of reducing words to their base or dictionary form. It treats different forms of a word as the same item, which is useful in counting, searching, or categorizing.\n",
    "    - In this project, WordNetLemmatizer is used to lemmatize each word in the \"emotions\" and \"foods\". So all words could convert to its base form\n",
    "    - Example: sadness -> sad, boredome -> bore/bored\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Filter all common words\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',''])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "food_count = {}\n",
    "\n",
    "\"\"\"\n",
    "    Finds the top comfort foods associated with a given mood.\n",
    "\n",
    "    emotion (str): The emotion to search for.\n",
    "    food_recommend (DataFrame): DataFrame containing 'Food Types' and 'Emotions' columns.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of the top comfort foods for the given mood.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# Processing\n",
    "def preprocess_text(emotion, food_recommend):\n",
    "\n",
    "    # Looping through the food data\n",
    "    for i in range(len(food_recommend)):\n",
    "\n",
    "        # PROCESS \"comfort_food_reasons\"\n",
    "        emotions_item = food_recommend[\"Emotions\"][i]\n",
    "        # Convert all items in comfort_food_reasons to str included NaN value.\n",
    "        # Split it into individual words, removes punctuation (. ,) and converts to lowercase\n",
    "        # checks if each word is not a stop word. (and with NLTK, common words will be removed such as \"I\",\"and\")\n",
    "        if isinstance(emotions_item, str):\n",
    "            emotions = emotions_item.lower().split()\n",
    "            emotions = [lemmatizer.lemmatize(word.strip('.,')) for word in emotions if word not in stop]\n",
    "\n",
    "        # PROCESS \"comfort_food\"\n",
    "        # If the mood is found, the processed similarly: split into item, punctuation removed, converted to lowercase, and lemmatized\n",
    "        if emotion in emotions:\n",
    "            foods = food_recommend[\"Food Types\"][i].lower().split(',')\n",
    "            foods = [lemmatizer.lemmatize(food.strip().strip('.,')) for food in foods if food not in stop]\n",
    "\n",
    "        # Add process food to food count and count food\n",
    "        # If the item is new to the dictionary, added with a count of 1; if it already exists, its count is incremented\n",
    "            for itemfood in foods:\n",
    "                if itemfood not in food_count.keys():\n",
    "                     food_count[itemfood] = 1 \n",
    "                else:\n",
    "                     food_count[itemfood] += 1\n",
    "\n",
    "    # Now specified mood is already associated with food.\n",
    "    # Sorting and selecting the top foods (most to least appearing food)\n",
    "    top_foods = sorted(food_count, key=food_count.get, reverse=True)[:10]\n",
    "    return top_foods\n",
    "\n",
    "\n",
    "def food_result(emotion):\n",
    "    topn = []\n",
    "    topn = preprocess_text(emotion, food_recommend) #function create dictionary only for particular mood\n",
    "    print(f\"10 Popular Comfort Foods in {emotion} are:\")\n",
    "    # print(topn[0])\n",
    "    # print(topn[1])\n",
    "    # print(topn[2]) \n",
    "    # print(topn[3]) \n",
    "    # print(topn[4]) \n",
    "    # print(topn[5]) \n",
    "    for food in topn:\n",
    "        print(food)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Popular Comfort Foods in sad are:\n",
      "ice cream\n",
      "pizza\n",
      "chip\n",
      "cheeseburger\n",
      "french fries\n",
      "fry\n",
      "cereal\n",
      "cooky\n",
      "chicken wings\n",
      "pasta\n"
     ]
    }
   ],
   "source": [
    "#food_result('bored')         10\n",
    "# food_result('blue')          0\n",
    "#food_result('yellow')        0\n",
    "#food_result('satisfaction')  3\n",
    "#food_result('late')           3\n",
    "food_result('sad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for restaurant dataset\n",
    "<a href=https://towardsdatascience.com/load-yelp-reviews-or-other-huge-json-files-with-ease-ad804c2f1537>Link For Load yelp review (huge json file) </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File dataset/yelp_academic_dataset_business.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\traml\\OneDrive\\Documents\\GitHub\\MoodMunch-Locator\\location_mood_recommendation.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/traml/OneDrive/Documents/GitHub/MoodMunch-Locator/location_mood_recommendation.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Convert yelp_business.json to csv\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/traml/OneDrive/Documents/GitHub/MoodMunch-Locator/location_mood_recommendation.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m restaurant_location \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(\u001b[39m'\u001b[39m\u001b[39mdataset/yelp_academic_dataset_business.json\u001b[39m\u001b[39m'\u001b[39m, lines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/traml/OneDrive/Documents/GitHub/MoodMunch-Locator/location_mood_recommendation.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m restaurant_location\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mdataset/restaurant_location.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/traml/OneDrive/Documents/GitHub/MoodMunch-Locator/location_mood_recommendation.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m restaurant_location \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdataset/restaurant_location.csv\u001b[39m\u001b[39m'\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\traml\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:780\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mif\u001b[39;00m convert_axes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    778\u001b[0m     convert_axes \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 780\u001b[0m json_reader \u001b[39m=\u001b[39m JsonReader(\n\u001b[0;32m    781\u001b[0m     path_or_buf,\n\u001b[0;32m    782\u001b[0m     orient\u001b[39m=\u001b[39morient,\n\u001b[0;32m    783\u001b[0m     typ\u001b[39m=\u001b[39mtyp,\n\u001b[0;32m    784\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m    785\u001b[0m     convert_axes\u001b[39m=\u001b[39mconvert_axes,\n\u001b[0;32m    786\u001b[0m     convert_dates\u001b[39m=\u001b[39mconvert_dates,\n\u001b[0;32m    787\u001b[0m     keep_default_dates\u001b[39m=\u001b[39mkeep_default_dates,\n\u001b[0;32m    788\u001b[0m     precise_float\u001b[39m=\u001b[39mprecise_float,\n\u001b[0;32m    789\u001b[0m     date_unit\u001b[39m=\u001b[39mdate_unit,\n\u001b[0;32m    790\u001b[0m     encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m    791\u001b[0m     lines\u001b[39m=\u001b[39mlines,\n\u001b[0;32m    792\u001b[0m     chunksize\u001b[39m=\u001b[39mchunksize,\n\u001b[0;32m    793\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[0;32m    794\u001b[0m     nrows\u001b[39m=\u001b[39mnrows,\n\u001b[0;32m    795\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m    796\u001b[0m     encoding_errors\u001b[39m=\u001b[39mencoding_errors,\n\u001b[0;32m    797\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    798\u001b[0m     engine\u001b[39m=\u001b[39mengine,\n\u001b[0;32m    799\u001b[0m )\n\u001b[0;32m    801\u001b[0m \u001b[39mif\u001b[39;00m chunksize:\n\u001b[0;32m    802\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n",
      "File \u001b[1;32mc:\\Users\\traml\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:893\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    891\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m filepath_or_buffer\n\u001b[0;32m    892\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mujson\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 893\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_data_from_filepath(filepath_or_buffer)\n\u001b[0;32m    894\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[1;32mc:\\Users\\traml\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:949\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    941\u001b[0m     filepath_or_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n\u001b[0;32m    942\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[0;32m    943\u001b[0m     \u001b[39misinstance\u001b[39m(filepath_or_buffer, \u001b[39mstr\u001b[39m)\n\u001b[0;32m    944\u001b[0m     \u001b[39mand\u001b[39;00m filepath_or_buffer\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[0;32m    948\u001b[0m ):\n\u001b[1;32m--> 949\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00mfilepath_or_buffer\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    950\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    951\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    952\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPassing literal json to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mread_json\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. To read from a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    957\u001b[0m     )\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File dataset/yelp_academic_dataset_business.json does not exist"
     ]
    }
   ],
   "source": [
    "# Convert yelp_business.json to csv\n",
    "restaurant_location = pd.read_json('dataset/yelp_academic_dataset_business.json', lines=True)\n",
    "restaurant_location.to_csv('dataset/restaurant_location.csv', index=False)\n",
    "\n",
    "restaurant_location = pd.read_csv('dataset/restaurant_location.csv', sep=',')\n",
    "# restaurant_location.info()\n",
    "df = restaurant_location[['city','state','latitude','longitude','stars']]\n",
    "df.query(\"`state`=='LA'\")\n",
    "#df.query(\"`state`=='OH'\")\n",
    "\n",
    "\n",
    "\n",
    "# a_pandas = []\n",
    "# r_dtypes = {}\n",
    "\n",
    "# with open('dataset/yelp_academic_dataset_business.json', 'r') as f:\n",
    "#     df = pd.read_json(f, orient=\"records\", lines=True, dtype=r_dtypes, chunksize=1000)\n",
    "\n",
    "        \n",
    "#     for chunk in df:\n",
    "#         reduced_chunk = chunk.drop(columns=['business_id', 'address','review_count', 'attributes','hours'])\\\n",
    "#                              .query(\"`state` == 'OH'\")\n",
    "                            \n",
    "#         a_pandas.append(reduced_chunk)\n",
    "    \n",
    "# a_pandas = pd.concat(a_pandas, ignore_index=True)\n",
    "# a_pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/yelp_academic_dataset_review.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\traml\\OneDrive\\Documents\\GitHub\\MoodMunch-Locator\\location_mood_recommendation.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/traml/OneDrive/Documents/GitHub/MoodMunch-Locator/location_mood_recommendation.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m b_pandas \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/traml/OneDrive/Documents/GitHub/MoodMunch-Locator/location_mood_recommendation.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m r_dtypes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mstars\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat16}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/traml/OneDrive/Documents/GitHub/MoodMunch-Locator/location_mood_recommendation.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdataset/yelp_academic_dataset_review.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/traml/OneDrive/Documents/GitHub/MoodMunch-Locator/location_mood_recommendation.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(f, orient\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m\"\u001b[39m, lines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, dtype\u001b[39m=\u001b[39mr_dtypes, chunksize\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/traml/OneDrive/Documents/GitHub/MoodMunch-Locator/location_mood_recommendation.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m df:\n",
      "File \u001b[1;32mc:\\Users\\traml\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/yelp_academic_dataset_review.json'"
     ]
    }
   ],
   "source": [
    "## unable to load Yelp reviews (other huge JSON yelp_review.json). Over 5gb\n",
    "# restaurant_review = pd.read_json('dataset/yelp_academic_dataset_review.json', lines=True, chunksize=1000 )\n",
    "# restaurant_review.to_csv('dataset/restaurant_review.csv', index=False)\n",
    "# restaurant_review\n",
    "\n",
    "\n",
    "b_pandas = []\n",
    "r_dtypes = {\"stars\": np.float16}\n",
    "\n",
    "with open('dataset/yelp_academic_dataset_review.json', 'r') as f:\n",
    "    df = pd.read_json(f, orient=\"records\", lines=True, dtype=r_dtypes, chunksize=1000)\n",
    "\n",
    "    for chunk in df:\n",
    "        reduced_chunk = chunk.drop(columns=['review_id', 'user_id', 'useful', 'funny', 'cool'])\\\n",
    "                             .query(\"`date` >= '2020-12-01'\").query(\"`stars` >= 3.9\")\n",
    "        b_pandas.append(reduced_chunk)\n",
    "    \n",
    "b_pandas = pd.concat(b_pandas, ignore_index=True)\n",
    "b_pandas\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
